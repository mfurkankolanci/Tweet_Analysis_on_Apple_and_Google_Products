{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECT TITLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing modules necessary for analysis and reading in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('judge-1377884607_tweet_product_company.csv',encoding= 'unicode_escape')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet_text'] = df['tweet_text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\"emotion_in_tweet_is_directed_at\": \"product\", \"is_there_an_emotion_directed_at_a_brand_or_product\": \"emotion\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweet_text', 'product', 'emotion'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google_words = ['Google','google', 'Android', 'android']\n",
    "# apple_words = ['Apple', 'apple', 'Iphone', 'iphone', 'Ipad', 'ipad','iPad 2','iPad','ipad2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def googlefunc(tweet_text):\n",
    "#     for word in google_words:\n",
    "#         if word in tweet_text:\n",
    "#             return True\n",
    "#     return False\n",
    "# def applefunc(tweet_text):\n",
    "#     for word in apple_words:\n",
    "#         if word in tweet_text:\n",
    "#             return True\n",
    "#     return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>product</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text             product  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...              iPhone   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...  iPad or iPhone App   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...                iPad   \n",
       "3  @sxsw I hope this year's festival isn't as cra...  iPad or iPhone App   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...              Google   \n",
       "\n",
       "            emotion  \n",
       "0  Negative emotion  \n",
       "1  Positive emotion  \n",
       "2  Positive emotion  \n",
       "3  Negative emotion  \n",
       "4  Positive emotion  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"product\"].fillna(\"none\", inplace = True)\n",
    "df_none = df.loc[df[\"product\"] == 'none']\n",
    "\n",
    "for word in [\"Apple\",]:\n",
    "    temp_df = df_none[df_none['tweet_text'].str.contains(word)]\n",
    "    temp_df['product'].replace({'none': word}, inplace=True)\n",
    "    df_none = temp_df.combine_first(df_none)\n",
    "    temp_df = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"product\"].fillna(\"none\", inplace = True)\n",
    "df_none = df.loc[df[\"product\"] == 'none']\n",
    "\n",
    "Apple_fill = df_none[df_none['tweet_text'].str.contains(\"Apple\")]\n",
    "Apple_fill['product'].replace({'none': 'Apple'}, inplace=True)\n",
    "df_none = Apple_fill.combine_first(df_none)\n",
    "\n",
    "Google_fill = df_none[df_none['tweet_text'].str.contains(\"Google\")]\n",
    "Google_fill['product'].replace({'none': 'Google'}, inplace=True)\n",
    "df_none = Google_fill.combine_first(df_none)\n",
    "\n",
    "Android_fill = df_none[df_none['tweet_text'].str.contains(\"Android\")]\n",
    "Android_fill['product'].replace({'none': 'Android'}, inplace=True)\n",
    "df_none = Android_fill.combine_first(df_none)\n",
    "\n",
    "iphone_fill = df_none[df_none['tweet_text'].str.contains('iphone')]\n",
    "iphone_fill['product'].replace({'none': 'Apple'}, inplace=True)\n",
    "df_none = iphone_fill.combine_first(df_none)\n",
    "\n",
    "apple_fill = df_none[df_none['tweet_text'].str.contains('apple')]\n",
    "apple_fill['product'].replace({'none': 'Apple'}, inplace=True)\n",
    "df_none = apple_fill.combine_first(df_none)\n",
    "\n",
    "ipad_fill = df_none[df_none['tweet_text'].str.contains('ipad')]\n",
    "ipad_fill['product'].replace({'none': 'Apple'}, inplace=True)\n",
    "df_none = ipad_fill.combine_first(df_none)\n",
    "\n",
    "google_fill = df_none[df_none['tweet_text'].str.contains('google')]\n",
    "google_fill['product'].replace({'none': 'Google'}, inplace=True)\n",
    "df_none = google_fill.combine_first(df_none)\n",
    "\n",
    "android_fill = df_none[df_none['tweet_text'].str.contains('android')]\n",
    "android_fill['product'].replace({'none': 'Android'}, inplace=True)\n",
    "df_none = android_fill.combine_first(df_none)\n",
    "\n",
    "ipad2_fill = df_none[df_none['tweet_text'].str.contains('ipad2')]\n",
    "ipad2_fill['product'].replace({'none': 'Apple'}, inplace=True)\n",
    "df_none = ipad2_fill.combine_first(df_none)\n",
    "\n",
    "iPad2_fill = df_none[df_none['tweet_text'].str.contains('iPad 2')]\n",
    "iPad2_fill['product'].replace({'none': 'Apple'}, inplace=True)\n",
    "df_none = iPad2_fill.combine_first(df_none)\n",
    "\n",
    "df_1 = df_none.combine_first(df)\n",
    "\n",
    "change_ipad = df_1[df_1['product'].str.contains('iPad')]\n",
    "change_ipad['product'].replace({'iPad': 'Apple'}, inplace=True)\n",
    "df_1 = change_ipad.combine_first(df_1)\n",
    "\n",
    "change_i = df_1[df_1['product'].str.contains('iPad or iPhone App')]\n",
    "change_i['product'].replace({'iPad or iPhone App': 'Apple'}, inplace=True)\n",
    "df_1 = change_i.combine_first(df_1)\n",
    "\n",
    "change_ip = df_1[df_1['product'].str.contains('iPhone')]\n",
    "change_ip['product'].replace({'iPhone': 'Apple'}, inplace=True)\n",
    "df_1 = change_ip.combine_first(df_1)\n",
    "\n",
    "change_g = df_1[df_1['product'].str.contains('Other Google product or service')]\n",
    "change_g['product'].replace({'Other Google product or service': 'Google'}, inplace=True)\n",
    "df_1 = change_g.combine_first(df_1)\n",
    "\n",
    "change_aa = df_1[df_1['product'].str.contains('Android App')]\n",
    "change_aa['product'].replace({'Android App': 'Android'}, inplace=True)\n",
    "df_1 = change_aa.combine_first(df_1)\n",
    "\n",
    "change_ap = df_1[df_1['product'].str.contains('Other Apple product or service')]\n",
    "change_ap['product'].replace({'Other Apple product or service': 'Apple'}, inplace=True)\n",
    "df_1 = change_ap.combine_first(df_1)\n",
    "\n",
    "ipo_fill = df_1[df_1['tweet_text'].str.contains('iPhone')]\n",
    "ipo_fill['product'].replace({'none': 'Apple'}, inplace=True)\n",
    "df_1 = ipo_fill.combine_first(df_1)\n",
    "\n",
    "ipa_fill = df_1[df_1['tweet_text'].str.contains('iPad')]\n",
    "ipa_fill['product'].replace({'none': 'Apple'}, inplace=True)\n",
    "df_1 = ipa_fill.combine_first(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df_1[df_1['product'] != \"none\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "android_join  = df_1[df_1['product'].str.contains(\"Android\")]\n",
    "android_join['product'].replace({\"Android\": 'Google'}, inplace=True)\n",
    "df_1 = android_join.combine_first(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1['product'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# function to clean text\n",
    "def cleaner(tweet):\n",
    "    tweet = re.sub(r'@[A-Za-z0-9]+','', str(tweet)) # removes @\n",
    "    tweet = re.sub(r'#', '', str(tweet)) # removes hashtag\n",
    "    tweet = re.sub(r'RT[\\s]+','', str(tweet)) # removes RT\n",
    "    tweet = re.sub(r'https?:\\/\\/\\S+', '', tweet) # remove hyperlink in tweet\n",
    "    tweet = re.sub(r'[^\\w\\s]', '', tweet) # removes punctuations\n",
    "    #tweet = re.sub(r'[^a-zA-Z]','', tweet)\n",
    "    return tweet\n",
    "\n",
    "df_1['tweet_text'] = df_1['tweet_text'].apply(cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"(?u)\\w{3,}[a-zA-Z]\")\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_text(text, tokenizer, stopwords_list, lemmatizer):\n",
    "#     lowered = text.lower()\n",
    "#     tokens = tokenizer.tokenize(lowered)\n",
    "#     stopped_tokens = [word for word in tokens if word not in stopwords_list]\n",
    "#     lemma = [lemmatizer.lemmatize(token) for token in stopped_tokens]\n",
    "#     return lemma\n",
    "\n",
    "#     # preprocessed = pd.concat(tokens, stopped_tokens, lemma)\n",
    "def stem_and_tokenize(document):\n",
    "    tokens = tokenizer.tokenize(document)\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_data = df_1.tweet_text.apply(lambda x: preprocess_text(x, tokenizer, stopwords_list, lemmatizer))\n",
    "# text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_1['tweet_text'] = text_data\n",
    "# df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df_1[(df_1['emotion'] == 'Positive emotion') | (df_1['emotion'] == 'Negative emotion')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1['emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1['emotion'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1['emotion'] = df_1['emotion'].apply(lambda x: 1 if x == \"Positive emotion\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df_1.drop('emotion',axis=1)\n",
    "y = df_1['emotion']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant vectorizer class\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Instantiate a vectorizer with max_features=10\n",
    "# (we are using the default token pattern)\n",
    "tfidf = TfidfVectorizer(max_features=400, stop_words=stopwords_list, tokenizer=stem_and_tokenize, strip_accents='ascii')\n",
    "\n",
    "# Fit the vectorizer on X_train[\"text\"] and transform it\n",
    "X_train_vectorized = tfidf.fit_transform(X_train[\"tweet_text\"])\n",
    "\n",
    "# Visually inspect the vectorized data\n",
    "X_train_df = pd.DataFrame.sparse.from_spmatrix(X_train_vectorized, columns=tfidf.get_feature_names())\n",
    "\n",
    "# Fit the vectorizer on X_train[\"text\"] and transform it\n",
    "X_test_vectorized = tfidf.transform(X_test[\"tweet_text\"])\n",
    "\n",
    "# Visually inspect the vectorized data\n",
    "X_test_df = pd.DataFrame.sparse.from_spmatrix(X_test_vectorized, columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant class and function\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Instantiate a MultinomialNB classifier\n",
    "baseline_model = MultinomialNB()\n",
    "baseline_model.fit(X_train_df,y_train)\n",
    "y_train_pred = baseline_model.predict(X_train_df)\n",
    "y_test_pred = baseline_model.predict(X_test_df)\n",
    "\n",
    "baseline_cv = cross_val_score(baseline_model, X_test_df, y_test)\n",
    "baseline_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix, classification_report\n",
    "def model_reports(model):\n",
    "    predictions = model.predict(X_test_df)\n",
    "    print(classification_report(y_test,predictions))\n",
    "    plot_confusion_matrix(model,X_test_df,y_test)\n",
    "model_reports(baseline_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# logreg = LogisticRegression(class_weight='balanced')\n",
    "# logreg.fit(X_train_df,y_train)\n",
    "# y_train_pred = baseline_model.predict(X_train_df)\n",
    "# y_test_pred = baseline_model.predict(X_test_df)\n",
    "\n",
    "# baseline_cv = cross_val_score(baseline_model, X_test_df, y_test)\n",
    "# baseline_cv\n",
    "\n",
    "# model_reports(logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "ratios = [0.25, 0.33, 0.5, 0.7, 1]\n",
    "\n",
    "for ratio in ratios:\n",
    "    smote = SMOTE(sampling_strategy=ratio)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_df, y_train) \n",
    "    logreg = LogisticRegression(fit_intercept=False, C=1e20, solver ='lbfgs')\n",
    "    logreg.fit(X_train_resampled, y_train_resampled)\n",
    "    model_reports(logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citations"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c94be41889154d41bf43aca8d1a8d1cd64b97c119170e03e2ed46ca87183f0c5"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
